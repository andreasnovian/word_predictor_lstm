{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drive access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "def downloadFile(fileId):\n",
    "  file = fileId\n",
    "  downloaded = drive.CreateFile({'id': fileId})\n",
    "  downloaded.GetContentFile(file)\n",
    "  return file\n",
    "\n",
    "def uploadToDrive(filename): \n",
    "  # upload file to drive\n",
    "  # link to folder : https://drive.google.com/drive/folders/1hQ1Oo1NZJyiKNcqg9DW0ARI7jdo4W-VG?usp=sharing\n",
    "  folder_id = '1hQ1Oo1NZJyiKNcqg9DW0ARI7jdo4W-VG'\n",
    "  file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n",
    "  file.SetContentFile(filename)\n",
    "  file.Upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing ./data\\dummy-europarl-v7.en\n",
      "Skipped 13065 lines\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "from io import open\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# dummy data\n",
    "data = os.path.join(DATA_DIR,\"dummy-europarl-v7.en\")\n",
    "preprocessed_data = os.path.join(DATA_DIR,\"dummy_preprocessed.txt\")\n",
    "\n",
    "# real data\n",
    "# data = os.path.join(DATA_DIR,\"europarl-v7.en\")\n",
    "# preprocessed_data = os.path.join(DATA_DIR,\"europarl-v7.en.preprocessed.txt\")\n",
    "\n",
    "NUM = ''\n",
    "\n",
    "# EOS_PUNCTS = {\".\": \".PERIOD\", \"?\": \"?QUESTIONMARK\", \"!\": \"!EXCLAMATIONMARK\"}\n",
    "# INS_PUNCTS = {\",\": \",COMMA\", \";\": \";SEMICOLON\", \":\": \":COLON\", \"-\": \"-DASH\"}\n",
    "\n",
    "EOS_PUNCTS = {\".\": \"\", \"?\": \"\", \"!\": \"\"}\n",
    "INS_PUNCTS = {\",\": \"\", \";\": \"\", \":\": \"\", \"-\": \"\"}\n",
    "\n",
    "forbidden_symbols = re.compile(r\"[\\[\\]\\(\\)\\/\\\\\\>\\<\\=\\+\\_\\*]\")\n",
    "apostrophe = re.compile(r\" '[^ ]\")\n",
    "numbers = re.compile(r\"\\d\")\n",
    "multiple_punct = re.compile(r'([\\.\\?\\!\\,\\:\\;\\-])(?:[\\.\\?\\!\\,\\:\\;\\-]){1,}')\n",
    "\n",
    "is_number = lambda x: len(numbers.sub(\"\", x)) / len(x) < 0.6\n",
    "\n",
    "def untokenize(line):\n",
    "    return line.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\"can not\", \"cannot\")\n",
    "\n",
    "def skip(line):\n",
    "\n",
    "    # skip empty line\n",
    "    if line.strip() == '':\n",
    "        return True\n",
    "\n",
    "    # skip line without one of EOS_PUNCTS\n",
    "    last_symbol = line[-1]\n",
    "    if not last_symbol in EOS_PUNCTS:\n",
    "        return True\n",
    "\n",
    "    # skip line with forbidden symbols\n",
    "    if forbidden_symbols.search(line) is not None:\n",
    "        return True\n",
    "    \n",
    "    # skip line with single quoted text, like 'high-risk'\n",
    "    if apostrophe.search(line) is not None:\n",
    "        return True\n",
    "    \n",
    "    # skip line with single quote\n",
    "    if \"\\' s \" in line:\n",
    "        return True\n",
    "    if \"\\' ll \" in line:\n",
    "        return True\n",
    "    if \"\\' ve \" in line:\n",
    "        return True\n",
    "    if \"\\' m \" in line:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def process_line(line):\n",
    "\n",
    "    tokens = word_tokenize(line)\n",
    "    output_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        if token in INS_PUNCTS:\n",
    "#             output_tokens.append(INS_PUNCTS[token])\n",
    "        elif token in EOS_PUNCTS:\n",
    "#             output_tokens.append(EOS_PUNCTS[token])\n",
    "        elif is_number(token):\n",
    "#             output_tokens.append(NUM)\n",
    "        else:\n",
    "            output_tokens.append(token.lower())\n",
    "\n",
    "    return untokenize(\" \".join(output_tokens) + \" \")\n",
    "\n",
    "def preprocess(input_file, output_file):\n",
    "    skipped = 0\n",
    "    \n",
    "    print(\"Preprocessing\", input_file)\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_txt:\n",
    "        with open(input_file, 'r', encoding='utf-8') as text:\n",
    "\n",
    "            for line in text:\n",
    "\n",
    "                line = line.replace(\"\\\"\", \"\").strip()\n",
    "                line = multiple_punct.sub(r\"\\g<1>\", line)\n",
    "\n",
    "                if skip(line):\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                line = process_line(line)\n",
    "\n",
    "                out_txt.write(line + '\\n')\n",
    "\n",
    "    print(\"Skipped %d lines\" % skipped)\n",
    "    \n",
    "\n",
    "preprocess(data, preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train dev test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1655208, 1)\n",
      "(204347, 1)\n",
      "(183913, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def writeToFile(data, filename):\n",
    "    data.to_csv(filename, index = False, header = False, quoting=csv.QUOTE_MINIMAL, sep=\"\\n\")\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "trainDevTest_name = \"dummy_ep\"\n",
    "# trainDevTest_name = \"ep\"\n",
    "trainDevTest_dir = os.path.join(DATA_DIR, trainDevTest_name)\n",
    "\n",
    "input_file = preprocessed_data\n",
    "\n",
    "df = pd.read_csv(input_file, sep=\"\\n\", header=None)\n",
    "# print(df.head())\n",
    "\n",
    "train, dev = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "train, test = train_test_split(train, test_size=0.1, shuffle=False)\n",
    "\n",
    "# print(train.head())\n",
    "# print(dev.head())\n",
    "# print(test.head())\n",
    "\n",
    "print(train.shape)\n",
    "print(dev.shape)\n",
    "print(test.shape)\n",
    "\n",
    "writeToFile(train, trainDevTest_dir+\".train.txt\")\n",
    "writeToFile(dev, trainDevTest_dir+\".dev.txt\")\n",
    "writeToFile(test, trainDevTest_dir+\".test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabulary and embedding for train dev test (Data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path './data\\embeddedData' already exists. Do you want to:\n",
      "[r]eplace the files in existing data path?\n",
      "[e]xit?\n",
      ">r\n",
      "Vocabulary size: 17067\n",
      "0.45% UNK-s in ./data\\embeddedData\\train\n",
      "1.21% UNK-s in ./data\\embeddedData\\dev\n",
      "1.34% UNK-s in ./data\\embeddedData\\test\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import operator\n",
    "try:\n",
    "    import cPickle\n",
    "except ImportError:\n",
    "    import _pickle as cPickle\n",
    "try:\n",
    "    input = raw_input\n",
    "except NameError:\n",
    "    pass\n",
    "from io import open\n",
    "import fnmatch\n",
    "import shutil\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_DIR,\"embeddedData\")\n",
    "\n",
    "# path to text file in the format:\n",
    "# word1 0.123 0.123 ... 0.123\n",
    "# word2 0.123 0.123 ... 0.123 etc...\n",
    "# e.g. glove.6B.50d.txt\n",
    "PRETRAINED_EMBEDDINGS_PATH = None\n",
    "\n",
    "END = \"</S>\"\n",
    "UNK = \"<UNK>\"\n",
    "NUM = \"<NUM>\"\n",
    "\n",
    "SPACE = \"_SPACE\"\n",
    "\n",
    "MAX_WORD_VOCABULARY_SIZE = 100000\n",
    "MIN_WORD_COUNT_IN_VOCAB = 2\n",
    "MAX_SEQUENCE_LEN = 50\n",
    "\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"train\")\n",
    "DEV_FILE = os.path.join(DATA_PATH, \"dev\")\n",
    "TEST_FILE = os.path.join(DATA_PATH, \"test\")\n",
    "\n",
    "# Stage 2\n",
    "TRAIN_FILE2 = os.path.join(DATA_PATH, \"train2\")\n",
    "DEV_FILE2 = os.path.join(DATA_PATH, \"dev2\")\n",
    "TEST_FILE2 = os.path.join(DATA_PATH, \"test2\")\n",
    "\n",
    "WORD_VOCAB_FILE = os.path.join(DATA_PATH, \"vocabulary\")\n",
    "\n",
    "PUNCTUATION_VOCABULARY = [SPACE, \",COMMA\", \".PERIOD\", \"?QUESTIONMARK\", \"!EXCLAMATIONMARK\", \":COLON\", \";SEMICOLON\", \"-DASH\"]\n",
    "PUNCTUATION_MAPPING = {}\n",
    "\n",
    "# Comma, period & question mark only:\n",
    "# PUNCTUATION_VOCABULARY = {SPACE, \",COMMA\", \".PERIOD\", \"?QUESTIONMARK\"}\n",
    "# PUNCTUATION_MAPPING = {\"!EXCLAMATIONMARK\": \".PERIOD\", \":COLON\": \",COMMA\", \";SEMICOLON\": \".PERIOD\", \"-DASH\": \",COMMA\"}\n",
    "\n",
    "EOS_TOKENS = {\".PERIOD\", \"?QUESTIONMARK\", \"!EXCLAMATIONMARK\"}\n",
    "CRAP_TOKENS = {\"<doc>\", \"<doc.>\"} # punctuations that are not included in vocabulary nor mapping, must be added to CRAP_TOKENS\n",
    "PAUSE_PREFIX = \"<sil=\"\n",
    "\n",
    "# replacement for pickling that takes less RAM. Useful for large datasets.\n",
    "def dump(d, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for s in d:\n",
    "            f.write(\"%s\\n\" % repr(s))\n",
    "\n",
    "def loadData(path):\n",
    "    d = []\n",
    "    with open(path, 'r') as f:\n",
    "        for l in f:\n",
    "            d.append(eval(l))\n",
    "    return d\n",
    "\n",
    "def add_counts(word_counts, line):\n",
    "    for w in line.split():\n",
    "        if w in CRAP_TOKENS or w in PUNCTUATION_VOCABULARY or w in PUNCTUATION_MAPPING or w.startswith(PAUSE_PREFIX):\n",
    "            continue\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "def build_vocabulary(word_counts):\n",
    "    return [wc[0] for wc in reversed(sorted(word_counts.items(), key=operator.itemgetter(1))) if wc[1] >= MIN_WORD_COUNT_IN_VOCAB and wc[0] != UNK][:MAX_WORD_VOCABULARY_SIZE] # Unk will be appended to end\n",
    "\n",
    "def write_vocabulary(vocabulary, file_name):\n",
    "    if END not in vocabulary:\n",
    "        vocabulary.append(END)\n",
    "    if UNK not in vocabulary:\n",
    "        vocabulary.append(UNK)\n",
    "\n",
    "    print(\"Vocabulary size: %d\" % len(vocabulary))\n",
    "\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(vocabulary))\n",
    "\n",
    "def iterable_to_dict(arr):\n",
    "    return dict((x.strip(), i) for (i, x) in enumerate(arr))\n",
    "\n",
    "def read_vocabulary(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return iterable_to_dict(f.readlines())\n",
    "\n",
    "def write_processed_dataset(input_files, output_file):\n",
    "    \"\"\"\n",
    "    data will consist of two sets of aligned subsequences (words and punctuations) of MAX_SEQUENCE_LEN tokens (actually punctuation sequence will be 1 element shorter).\n",
    "    If a sentence is cut, then it will be added to next subsequence entirely (words before the cut belong to both sequences)\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    word_vocabulary = read_vocabulary(WORD_VOCAB_FILE)\n",
    "    punctuation_vocabulary = iterable_to_dict(PUNCTUATION_VOCABULARY)\n",
    "\n",
    "    num_total = 0\n",
    "    num_unks = 0\n",
    "\n",
    "    current_words = []\n",
    "    current_punctuations = []\n",
    "    current_pauses = []\n",
    "\n",
    "    last_eos_idx = 0 # if it's still 0 when MAX_SEQUENCE_LEN is reached, then the sentence is too long and skipped.\n",
    "    last_token_was_punctuation = True # skipt first token if it's punctuation\n",
    "    last_pause = 0.0\n",
    "\n",
    "    skip_until_eos = False # if a sentence does not fit into subsequence, then we need to skip tokens until we find a new sentence\n",
    "\n",
    "    for input_file in input_files:\n",
    "\n",
    "        with open(input_file, 'r', encoding='utf-8') as text:\n",
    "\n",
    "            for line in text:\n",
    "\n",
    "                for token in line.split():\n",
    "\n",
    "                    # First map oov punctuations to known punctuations\n",
    "                    if token in PUNCTUATION_MAPPING:\n",
    "                        token = PUNCTUATION_MAPPING[token]\n",
    "\n",
    "                    if skip_until_eos:\n",
    "\n",
    "                        if token in EOS_TOKENS:\n",
    "                            skip_until_eos = False\n",
    "\n",
    "                        continue\n",
    "\n",
    "                    elif token in CRAP_TOKENS:\n",
    "                        continue\n",
    "\n",
    "                    elif token.startswith(PAUSE_PREFIX):\n",
    "                        last_pause = float(token.replace(PAUSE_PREFIX,\"\").replace(\">\",\"\"))\n",
    "\n",
    "                    elif token in punctuation_vocabulary:\n",
    "\n",
    "                        if last_token_was_punctuation: # if we encounter sequences like: \"... !EXLAMATIONMARK .PERIOD ...\", then we only use the first punctuation and skip the ones that follow\n",
    "                            continue\n",
    "\n",
    "                        if token in EOS_TOKENS:\n",
    "                            last_eos_idx = len(current_punctuations) # no -1, because the token is not added yet\n",
    "\n",
    "                        punctuation = punctuation_vocabulary[token]\n",
    "\n",
    "                        current_punctuations.append(punctuation)\n",
    "                        last_token_was_punctuation = True\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        if not last_token_was_punctuation:\n",
    "                            current_punctuations.append(punctuation_vocabulary[SPACE])\n",
    "\n",
    "                        word = word_vocabulary.get(token, word_vocabulary[UNK])\n",
    "\n",
    "                        current_words.append(word)\n",
    "                        current_pauses.append(last_pause)\n",
    "                        last_token_was_punctuation = False\n",
    "\n",
    "                        num_total += 1\n",
    "                        num_unks += int(word == word_vocabulary[UNK])\n",
    "\n",
    "                    if len(current_words) == MAX_SEQUENCE_LEN: # this also means, that last token was a word\n",
    "                        \n",
    "                        assert len(current_words) == len(current_punctuations) + 1, \"#words: %d; #punctuations: %d\" % (len(current_words), len(current_punctuations))\n",
    "                        assert current_pauses == [] or len(current_words) == len(current_pauses), \"#words: %d; #pauses: %d\" % (len(current_words), len(current_pauses))\n",
    "\n",
    "                        # Sentence did not fit into subsequence - skip it\n",
    "                        if last_eos_idx == 0: \n",
    "                            skip_until_eos = True\n",
    "\n",
    "                            current_words = []\n",
    "                            current_punctuations = []\n",
    "                            current_pauses = []\n",
    "\n",
    "                            last_token_was_punctuation = True # next sequence starts with a new sentence, so is preceded by eos which is punctuation\n",
    "\n",
    "                        else:\n",
    "                            subsequence = [\n",
    "                                current_words[:-1] + [word_vocabulary[END]],\n",
    "                                current_punctuations,\n",
    "                                current_pauses[1:]\n",
    "                            ]\n",
    "\n",
    "                            data.append(subsequence)\n",
    "\n",
    "                            # Carry unfinished sentence to next subsequence\n",
    "                            current_words = current_words[last_eos_idx+1:]\n",
    "                            current_punctuations = current_punctuations[last_eos_idx+1:]\n",
    "                            current_pauses = current_pauses[last_eos_idx+1:]\n",
    "\n",
    "                        last_eos_idx = 0 # sequence always starts with a new sentence\n",
    "\n",
    "    print(\"%.2f%% UNK-s in %s\" % (num_unks / num_total * 100, output_file))\n",
    "\n",
    "    dump(data, output_file)\n",
    "\n",
    "def create_dev_test_train_split_and_vocabulary(root_path, create_vocabulary, train_output, dev_output, test_output, pretrained_embeddings_path=None):\n",
    "\n",
    "    train_txt_files = []\n",
    "    dev_txt_files = []\n",
    "    test_txt_files = []\n",
    "\n",
    "    if create_vocabulary and not pretrained_embeddings_path:\n",
    "        word_counts = dict()\n",
    "    \n",
    "    for root, dirnames, filenames in os.walk(root_path):\n",
    "        for filename in fnmatch.filter(filenames, '*.txt'):\n",
    "\n",
    "            path = os.path.join(root, filename)\n",
    "\n",
    "            if filename.endswith(\".test.txt\"):\n",
    "                test_txt_files.append(path)\n",
    "\n",
    "            elif filename.endswith(\".dev.txt\"):\n",
    "                dev_txt_files.append(path)\n",
    "\n",
    "            else:\n",
    "                train_txt_files.append(path)\n",
    "\n",
    "                if create_vocabulary and not pretrained_embeddings_path:\n",
    "                    with open(path, 'r', encoding='utf-8') as text:\n",
    "                        for line in text:\n",
    "                            add_counts(word_counts, line)\n",
    "\n",
    "    if create_vocabulary:\n",
    "        if pretrained_embeddings_path:\n",
    "            vocabulary = []\n",
    "            embeddings = []\n",
    "            with open(pretrained_embeddings_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.split()\n",
    "                    w = line[0]\n",
    "                    e = [float(x) for x in line[1:]]\n",
    "                    vocabulary.append(w)\n",
    "                    embeddings.append(e)\n",
    "\n",
    "            with open(\"We.pcl\", 'wb') as f:\n",
    "                cPickle.dump(embeddings, f, cPickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            vocabulary = build_vocabulary(word_counts)\n",
    "        write_vocabulary(vocabulary, WORD_VOCAB_FILE)\n",
    "\n",
    "    write_processed_dataset(train_txt_files, train_output)\n",
    "    write_processed_dataset(dev_txt_files, dev_output)\n",
    "    write_processed_dataset(test_txt_files, test_output)\n",
    "    \n",
    "    \n",
    "# path to train, dev, and test files\n",
    "path = os.path.join(DATA_DIR,\"trainDevTest\")\n",
    "\n",
    "replace = False\n",
    "if os.path.exists(DATA_PATH):\n",
    "\n",
    "    while True:\n",
    "        resp = input(\"Data path '%s' already exists. Do you want to:\\n[r]eplace the files in existing data path?\\n[e]xit?\\n>\" % DATA_PATH)\n",
    "        resp = resp.lower().strip()\n",
    "        if resp not in ('r', 'e'):\n",
    "            continue\n",
    "        if resp == 'e':\n",
    "            sys.exit()\n",
    "        elif resp == 'r':\n",
    "            replace = True\n",
    "        break\n",
    "\n",
    "if replace and os.path.exists(DATA_PATH):\n",
    "    shutil.rmtree(DATA_PATH)\n",
    "\n",
    "os.makedirs(DATA_PATH)\n",
    "    \n",
    "create_dev_test_train_split_and_vocabulary(path, True, TRAIN_FILE, DEV_FILE, TEST_FILE, PRETRAINED_EMBEDDINGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "try:\n",
    "    import cPickle\n",
    "    cpickle_options = {}\n",
    "except ImportError:\n",
    "    import _pickle as cPickle\n",
    "    cpickle_options = { 'encoding': 'latin-1' }\n",
    "import os\n",
    "import theano.tensor as T\n",
    "\n",
    "def PReLU(a, x):\n",
    "    return T.maximum(0.0, x) + a * T.minimum(0.0, x)\n",
    "\n",
    "def ReLU(x):\n",
    "    return T.maximum(0.0, x)\n",
    "\n",
    "def _get_shape(i, o, keepdims):\n",
    "    if (i == 1 or o == 1) and not keepdims:\n",
    "        return (max(i,o),)\n",
    "    else:\n",
    "        return (i, o)\n",
    "\n",
    "def _slice(tensor, size, i):\n",
    "    \"\"\"Gets slice of columns of the tensor\"\"\"\n",
    "    if tensor.ndim == 2:\n",
    "        return tensor[:, i*size:(i+1)*size]\n",
    "    elif tensor.ndim == 1:\n",
    "        return tensor[i*size:(i+1)*size]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Tensor should be 1 or 2 dimensional\")\n",
    "\n",
    "def weights_const(i, o, name, const, keepdims=False):\n",
    "    W_values = np.ones(_get_shape(i, o, keepdims)).astype(theano.config.floatX) * const\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def weights_identity(i, o, name, const, keepdims=False):\n",
    "    #\"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\" (2015) (http://arxiv.org/abs/1504.00941)\n",
    "    W_values = np.eye(*_get_shape(i, o, keepdims)).astype(theano.config.floatX) * const\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def weights_Glorot(i, o, name, rng, is_logistic_sigmoid=False, keepdims=False):\n",
    "    #http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "    d = np.sqrt(6. / (i + o))\n",
    "    if is_logistic_sigmoid:\n",
    "        d *= 4.\n",
    "    W_values = rng.uniform(low=-d, high=d, size=_get_shape(i, o, keepdims)).astype(theano.config.floatX)\n",
    "    return theano.shared(value=W_values, name=name, borrow=True)\n",
    "\n",
    "def loadModel(file_path, minibatch_size, x, p=None):\n",
    "    try:\n",
    "        import cPickle\n",
    "    except ImportError:\n",
    "        import _pickle as cPickle\n",
    "    import theano\n",
    "    import numpy as np\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        state = cPickle.load(f, **cpickle_options)\n",
    "\n",
    "    rng = np.random\n",
    "    rng.set_state(state[\"random_state\"])\n",
    "\n",
    "    net = GRU(\n",
    "        rng=rng,\n",
    "        x=x,\n",
    "        minibatch_size=minibatch_size,\n",
    "        n_hidden=state[\"n_hidden\"],\n",
    "        x_vocabulary=state[\"x_vocabulary\"],\n",
    "        y_vocabulary=state[\"y_vocabulary\"],\n",
    "        stage1_model_file_name=state.get(\"stage1_model_file_name\", None),\n",
    "        p=p\n",
    "        )\n",
    "\n",
    "    for net_param, state_param in zip(net.params, state[\"params\"]):\n",
    "        net_param.set_value(state_param, borrow=True)\n",
    "\n",
    "    gsums = [theano.shared(gsum) for gsum in state[\"gsums\"]] if state[\"gsums\"] else None\n",
    "\n",
    "    return net, (gsums, state[\"learning_rate\"], state[\"validation_ppl_history\"], state[\"epoch\"], rng)\n",
    "\n",
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self, rng, n_in, n_out, minibatch_size):\n",
    "        super(GRULayer, self).__init__()\n",
    "        # Notation from: An Empirical Exploration of Recurrent Network Architectures\n",
    "\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        # Initial hidden state\n",
    "        self.h0 = theano.shared(value=np.zeros((minibatch_size, n_out)).astype(theano.config.floatX), name='h0', borrow=True)\n",
    "\n",
    "        # Gate parameters:\n",
    "        self.W_x = weights_Glorot(n_in, n_out*2, 'W_x', rng)\n",
    "        self.W_h = weights_Glorot(n_out, n_out*2, 'W_h', rng)\n",
    "        self.b = weights_const(1, n_out*2, 'b', 0)\n",
    "        # Input parameters\n",
    "        self.W_x_h = weights_Glorot(n_in, n_out, 'W_x_h', rng)\n",
    "        self.W_h_h = weights_Glorot(n_out, n_out, 'W_h_h', rng)\n",
    "        self.b_h = weights_const(1, n_out, 'b_h', 0)\n",
    "\n",
    "        self.params = [self.W_x, self.W_h, self.b, self.W_x_h, self.W_h_h, self.b_h]\n",
    "\n",
    "    def step(self, x_t, h_tm1):\n",
    "\n",
    "        rz = T.nnet.sigmoid(T.dot(x_t, self.W_x) + T.dot(h_tm1, self.W_h) + self.b)\n",
    "        r = _slice(rz, self.n_out, 0)\n",
    "        z = _slice(rz, self.n_out, 1)\n",
    "\n",
    "        h = T.tanh(T.dot(x_t, self.W_x_h) + T.dot(h_tm1 * r, self.W_h_h) + self.b_h)\n",
    "\n",
    "        h_t = z * h_tm1 + (1. - z) * h\n",
    "\n",
    "        return h_t\n",
    "\n",
    "class GRU(object):\n",
    "\n",
    "    def __init__(self, rng, x, minibatch_size, n_hidden, x_vocabulary, y_vocabulary, stage1_model_file_name=None, p=None):\n",
    "\n",
    "        assert not stage1_model_file_name and not p, \"Stage 1 model can't have stage 1 model\"\n",
    "\n",
    "        x_vocabulary_size = len(x_vocabulary)\n",
    "        y_vocabulary_size = len(y_vocabulary)\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.x_vocabulary = x_vocabulary\n",
    "        self.y_vocabulary = y_vocabulary\n",
    "\n",
    "        # input model\n",
    "        pretrained_embs_path = \"We.pcl\"\n",
    "        if os.path.exists(pretrained_embs_path):\n",
    "            print(\"Found pretrained embeddings in '%s'. Using them...\" % pretrained_embs_path)\n",
    "            with open(pretrained_embs_path, 'rb') as f:\n",
    "                We = cPickle.load(f, **cpickle_options)\n",
    "            n_emb = len(We[0])\n",
    "            We.append([0.1]*n_emb) # END\n",
    "            We.append([0.0]*n_emb) # UNK - both quite arbitrary initializations\n",
    "\n",
    "            We = np.array(We).astype(theano.config.floatX)\n",
    "            self.We = theano.shared(value=We, name=\"We\", borrow=True)\n",
    "        else:\n",
    "            n_emb = n_hidden\n",
    "            self.We = weights_Glorot(x_vocabulary_size, n_emb, 'We', rng) # Share embeddings between forward and backward model\n",
    "\n",
    "        self.GRU_f = GRULayer(rng=rng, n_in=n_emb, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "        self.GRU_b = GRULayer(rng=rng, n_in=n_emb, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "\n",
    "        # output model\n",
    "        self.GRU = GRULayer(rng=rng, n_in=n_hidden*2, n_out=n_hidden, minibatch_size=minibatch_size)\n",
    "        self.Wy = weights_const(n_hidden, y_vocabulary_size, 'Wy', 0)\n",
    "        self.by = weights_const(1, y_vocabulary_size, 'by', 0)\n",
    "\n",
    "        # attention model\n",
    "        n_attention = n_hidden * 2 # to match concatenated forward and reverse model states\n",
    "        self.Wa_h = weights_Glorot(n_hidden, n_attention, 'Wa_h', rng) # output model previous hidden state to attention model weights\n",
    "        self.Wa_c = weights_Glorot(n_attention, n_attention, 'Wa_c', rng) # contexts to attention model weights\n",
    "        self.ba = weights_const(1, n_attention, 'ba', 0)\n",
    "        self.Wa_y = weights_Glorot(n_attention, 1, 'Wa_y', rng) # gives weights to contexts\n",
    "\n",
    "        # Late fusion parameters\n",
    "        self.Wf_h = weights_const(n_hidden, n_hidden, 'Wf_h', 0)\n",
    "        self.Wf_c = weights_const(n_attention, n_hidden, 'Wf_c', 0)\n",
    "        self.Wf_f = weights_const(n_hidden, n_hidden, 'Wf_f', 0)\n",
    "        self.bf = weights_const(1, n_hidden, 'by', 0)\n",
    "\n",
    "        self.params = [self.We,\n",
    "                       self.Wy, self.by,\n",
    "                       self.Wa_h, self.Wa_c, self.ba, self.Wa_y,\n",
    "                       self.Wf_h, self.Wf_c, self.Wf_f, self.bf]\n",
    "\n",
    "        self.params += self.GRU.params + self.GRU_f.params + self.GRU_b.params\n",
    "\n",
    "        # bi-directional recurrence\n",
    "        def input_recurrence(x_f_t, x_b_t, h_f_tm1, h_b_tm1):\n",
    "            h_f_t = self.GRU_f.step(x_t=x_f_t, h_tm1=h_f_tm1)\n",
    "            h_b_t = self.GRU_b.step(x_t=x_b_t, h_tm1=h_b_tm1)\n",
    "            return [h_f_t, h_b_t]\n",
    "\n",
    "        def output_recurrence(x_t, h_tm1, Wa_h, Wa_y, Wf_h, Wf_c, Wf_f, bf, Wy, by, context, projected_context):\n",
    "\n",
    "            # Attention model\n",
    "            h_a = T.tanh(projected_context + T.dot(h_tm1, Wa_h))\n",
    "            alphas = T.exp(T.dot(h_a, Wa_y))\n",
    "            alphas = alphas.reshape((alphas.shape[0], alphas.shape[1])) # drop 2-axis (sized 1)\n",
    "            alphas = alphas / alphas.sum(axis=0, keepdims=True)\n",
    "            weighted_context = (context * alphas[:,:,None]).sum(axis=0)\n",
    "\n",
    "            h_t = self.GRU.step(x_t=x_t, h_tm1=h_tm1)\n",
    "\n",
    "            # Late fusion\n",
    "            lfc = T.dot(weighted_context, Wf_c) # late fused context\n",
    "            fw = T.nnet.sigmoid(T.dot(lfc, Wf_f) + T.dot(h_t, Wf_h) + bf) # fusion weights\n",
    "            hf_t = lfc * fw + h_t # weighted fused context + hidden state\n",
    "\n",
    "            z = T.dot(hf_t, Wy) + by\n",
    "            y_t = T.nnet.softmax(z)\n",
    "\n",
    "            return [h_t, hf_t, y_t, alphas]\n",
    "\n",
    "        x_emb = self.We[x.flatten()].reshape((x.shape[0], minibatch_size, n_emb))\n",
    "\n",
    "        [h_f_t, h_b_t], _ = theano.scan(fn=input_recurrence,\n",
    "            sequences=[x_emb, x_emb[::-1]], # forward and backward sequences\n",
    "            outputs_info=[self.GRU_f.h0, self.GRU_b.h0])\n",
    "\n",
    "        # 0-axis is time steps, 1-axis is batch size and 2-axis is hidden layer size\n",
    "        context = T.concatenate([h_f_t, h_b_t[::-1]], axis=2)\n",
    "        projected_context = T.dot(context, self.Wa_c) + self.ba\n",
    "\n",
    "        [_, self.last_hidden_states, self.y, self.alphas], _ = theano.scan(fn=output_recurrence,\n",
    "            sequences=[context[1:]], # ignore the 1st word in context, because there's no punctuation before that\n",
    "            non_sequences=[self.Wa_h, self.Wa_y, self.Wf_h, self.Wf_c, self.Wf_f, self.bf, self.Wy, self.by, context, projected_context],\n",
    "            outputs_info=[self.GRU.h0, None, None, None])\n",
    "\n",
    "        print(\"Number of parameters is %d\" % sum(np.prod(p.shape.eval()) for p in self.params))\n",
    "\n",
    "        self.L1 = sum(abs(p).sum() for p in self.params)\n",
    "        self.L2_sqr = sum((p**2).sum() for p in self.params)\n",
    "\n",
    "    def cost(self, y):\n",
    "        num_outputs = self.y.shape[0]*self.y.shape[1] # time steps * number of parallel sequences in batch\n",
    "        output = self.y.reshape((num_outputs, self.y.shape[2]))\n",
    "        return -T.sum(T.log(output[T.arange(num_outputs), y.flatten()]))\n",
    "\n",
    "    def save(self, file_path, gsums=None, learning_rate=None, validation_ppl_history=None, best_validation_ppl=None, epoch=None, random_state=None):\n",
    "        try:\n",
    "            import cPickle\n",
    "        except ImportError:\n",
    "            import _pickle as cPickle\n",
    "        state = {\n",
    "            \"type\":                     self.__class__.__name__,\n",
    "            \"n_hidden\":                 self.n_hidden,\n",
    "            \"x_vocabulary\":             self.x_vocabulary,\n",
    "            \"y_vocabulary\":             self.y_vocabulary,\n",
    "            \"stage1_model_file_name\":   self.stage1_model_file_name if hasattr(self, \"stage1_model_file_name\") else None,\n",
    "            \"params\":                   [p.get_value(borrow=True) for p in self.params],\n",
    "            \"gsums\":                    [s.get_value(borrow=True) for s in gsums] if gsums else None,\n",
    "            \"learning_rate\":            learning_rate,\n",
    "            \"validation_ppl_history\":   validation_ppl_history,\n",
    "            \"epoch\":                    epoch,\n",
    "            \"random_state\":             random_state\n",
    "        }\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            cPickle.dump(state, f, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from time import time\n",
    "\n",
    "import theano\n",
    "try:\n",
    "    import cPickle\n",
    "except ImportError:\n",
    "    import _pickle as cPickle\n",
    "import sys\n",
    "import os.path\n",
    "try:\n",
    "    input = raw_input\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "MAX_EPOCHS = 50\n",
    "MINIBATCH_SIZE = 128\n",
    "L2_REG = 0.0\n",
    "CLIPPING_THRESHOLD = 2.0\n",
    "PATIENCE_EPOCHS = 1\n",
    "MAX_SEQUENCE_LEN = 50\n",
    "\n",
    "\"\"\"\n",
    "Bi-directional RNN with attention\n",
    "For a sequence of N words, the model makes N punctuation decisions (no punctuation before the first word, but there's a decision after the last word or before </S>)\n",
    "\"\"\"\n",
    "\n",
    "def get_minibatch(file_name, batch_size, shuffle, with_pauses=False):\n",
    "\n",
    "    dataset = loadData(file_name)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "    X_batch = []\n",
    "    Y_batch = []\n",
    "    if with_pauses:\n",
    "        P_batch = []\n",
    "\n",
    "    if len(dataset) < batch_size:\n",
    "        print(\"WARNING: Not enough samples in '%s'. Reduce mini-batch size to %d or use a dataset with at least %d words.\" % (\n",
    "            file_name,\n",
    "            len(dataset),\n",
    "            MINIBATCH_SIZE * MAX_SEQUENCE_LEN))\n",
    "\n",
    "    for subsequence in dataset:\n",
    "\n",
    "        X_batch.append(subsequence[0])\n",
    "        Y_batch.append(subsequence[1])\n",
    "        if with_pauses:\n",
    "            P_batch.append(subsequence[2])\n",
    "        \n",
    "        if len(X_batch) == batch_size:\n",
    "\n",
    "            # Transpose, because the model assumes the first axis is time\n",
    "            X = np.array(X_batch, dtype=np.int32).T\n",
    "            Y = np.array(Y_batch, dtype=np.int32).T\n",
    "            if with_pauses:\n",
    "                P = np.array(P_batch, dtype=theano.config.floatX).T\n",
    "            \n",
    "            if with_pauses:\n",
    "                yield X, Y, P\n",
    "            else:\n",
    "                yield X, Y\n",
    "\n",
    "            X_batch = []\n",
    "            Y_batch = []\n",
    "            if with_pauses:\n",
    "                P_batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 0.02 Model_ep_h256_lr0.02.pcl\n",
      "Building model...\n",
      "Number of parameters is 6406408\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andre\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\theano\\tensor\\subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "SPACE = \"_SPACE\"\n",
    "\n",
    "model_name = \"ep\"\n",
    "num_hidden = 256\n",
    "learning_rate = 0.02\n",
    "\n",
    "# constant from Data\n",
    "DATA_PATH = os.path.join(DATA_DIR,\"embeddedData\")\n",
    "WORD_VOCAB_FILE = os.path.join(DATA_PATH, \"vocabulary\")\n",
    "PUNCTUATION_VOCABULARY = [SPACE, \",COMMA\", \".PERIOD\", \"?QUESTIONMARK\", \"!EXCLAMATIONMARK\", \":COLON\", \";SEMICOLON\", \"-DASH\"]\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"train\")\n",
    "DEV_FILE = os.path.join(DATA_PATH, \"dev\")\n",
    "\n",
    "model_file_name = \"Model_%s_h%d_lr%s.pcl\" % (model_name, num_hidden, learning_rate)\n",
    "\n",
    "print(num_hidden, learning_rate, model_file_name)\n",
    "\n",
    "word_vocabulary = read_vocabulary(WORD_VOCAB_FILE)\n",
    "punctuation_vocabulary = iterable_to_dict(PUNCTUATION_VOCABULARY)\n",
    "\n",
    "x = T.imatrix('x')\n",
    "y = T.imatrix('y')\n",
    "lr = T.scalar('lr')\n",
    "\n",
    "continue_with_previous = False\n",
    "if os.path.isfile(model_file_name):\n",
    "\n",
    "    while True:\n",
    "        resp = input(\"Found an existing model with the name %s. Do you want to:\\n[c]ontinue training the existing model?\\n[r]eplace the existing model and train a new one?\\n[e]xit?\\n>\" % model_file_name)\n",
    "        resp = resp.lower().strip()\n",
    "        if resp not in ('c', 'r', 'e'):\n",
    "            continue\n",
    "        if resp == 'e':\n",
    "            sys.exit()\n",
    "        elif resp == 'c':\n",
    "            continue_with_previous = True\n",
    "        break\n",
    "\n",
    "if continue_with_previous:\n",
    "    print(\"Loading previous model state\")\n",
    "\n",
    "    net, state = loadModel(model_file_name, MINIBATCH_SIZE, x)\n",
    "    gsums, learning_rate, validation_ppl_history, starting_epoch, rng = state\n",
    "    best_ppl = min(validation_ppl_history)\n",
    "\n",
    "else:\n",
    "    rng = np.random\n",
    "    rng.seed(1)\n",
    "\n",
    "    print(\"Building model...\")\n",
    "    net = GRU(\n",
    "        rng=rng,\n",
    "        x=x,\n",
    "        minibatch_size=MINIBATCH_SIZE,\n",
    "        n_hidden=num_hidden,\n",
    "        x_vocabulary=word_vocabulary,\n",
    "        y_vocabulary=punctuation_vocabulary\n",
    "        )\n",
    "\n",
    "    starting_epoch = 0\n",
    "    best_ppl = np.inf\n",
    "    validation_ppl_history = []\n",
    "        \n",
    "    gsums = [theano.shared(np.zeros_like(param.get_value(borrow=True))) for param in net.params]\n",
    "\n",
    "cost = net.cost(y) + L2_REG * net.L2_sqr\n",
    "\n",
    "gparams = T.grad(cost, net.params)\n",
    "updates = OrderedDict()\n",
    "\n",
    "# Compute norm of gradients\n",
    "norm = T.sqrt(T.sum(\n",
    "            [T.sum(gparam ** 2) for gparam in gparams]\n",
    "        ))\n",
    "\n",
    "    \n",
    "# Adagrad: \"Adaptive subgradient methods for online learning and stochastic optimization\" (2011)    \n",
    "for gparam, param, gsum in zip(gparams, net.params, gsums):\n",
    "    gparam = T.switch(\n",
    "        T.ge(norm, CLIPPING_THRESHOLD),\n",
    "        gparam / norm * CLIPPING_THRESHOLD,\n",
    "        gparam\n",
    "    ) # Clipping of gradients\n",
    "    updates[gsum] = gsum + (gparam ** 2)\n",
    "    updates[param] = param - lr * (gparam / (T.sqrt(updates[gsum] + 1e-6)))\n",
    "\n",
    "train_model = theano.function(\n",
    "    inputs=[x, y, lr],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")\n",
    "\n",
    "validate_model = theano.function(\n",
    "    inputs=[x, y],\n",
    "    outputs=net.cost(y)\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(starting_epoch, MAX_EPOCHS):\n",
    "    t0 = time()\n",
    "    total_neg_log_likelihood = 0\n",
    "    total_num_output_samples = 0\n",
    "    iteration = 0 \n",
    "    for X, Y in get_minibatch(TRAIN_FILE, MINIBATCH_SIZE, shuffle=True):\n",
    "        total_neg_log_likelihood += train_model(X, Y, learning_rate)\n",
    "        total_num_output_samples += np.prod(Y.shape)\n",
    "        iteration += 1\n",
    "        if iteration % 100 == 0:\n",
    "            sys.stdout.write(\"PPL: %.4f; Speed: %.2f sps\\n\" % (np.exp(total_neg_log_likelihood / total_num_output_samples), total_num_output_samples / max(time() - t0, 1e-100)))\n",
    "            sys.stdout.flush()\n",
    "    print(\"Total number of training labels: %d\" % total_num_output_samples)\n",
    "\n",
    "    total_neg_log_likelihood = 0\n",
    "    total_num_output_samples = 0\n",
    "    for X, Y in get_minibatch(DEV_FILE, MINIBATCH_SIZE, shuffle=False):\n",
    "        total_neg_log_likelihood += validate_model(X, Y)\n",
    "        total_num_output_samples += np.prod(Y.shape)\n",
    "    print(\"Total number of validation labels: %d\" % total_num_output_samples)\n",
    "        \n",
    "    ppl = np.exp(total_neg_log_likelihood / total_num_output_samples)\n",
    "    validation_ppl_history.append(ppl)\n",
    "\n",
    "    print(\"Validation perplexity is %s\" % np.round(ppl, 4))\n",
    "\n",
    "    if ppl <= best_ppl:\n",
    "        best_ppl = ppl\n",
    "        net.save(model_file_name, gsums=gsums, learning_rate=learning_rate, validation_ppl_history=validation_ppl_history, best_validation_ppl=best_ppl, epoch=epoch, random_state=rng.get_state())\n",
    "    elif best_ppl not in validation_ppl_history[-PATIENCE_EPOCHS:]:\n",
    "        print(\"Finished!\")\n",
    "        print(\"Best validation perplexity was %s\" % best_ppl)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
